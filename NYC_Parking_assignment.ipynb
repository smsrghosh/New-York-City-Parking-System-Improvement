{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.SparkSession, The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark DataFrame and Sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrames by loading data file from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Summons Number='5092469481', Plate ID='GZH7067', Registration State='NY', Issue Date='2016-07-10', Violation Code='7', Vehicle Body Type='SUBN', Vehicle Make='TOYOT', Violation Precinct='0', Issuer Precinct='0', Violation Time='0143A'),\n",
       " Row(Summons Number='5092451658', Plate ID='GZH7067', Registration State='NY', Issue Date='2016-07-08', Violation Code='7', Vehicle Body Type='SUBN', Vehicle Make='TOYOT', Violation Precinct='0', Issuer Precinct='0', Violation Time='0400P'),\n",
       " Row(Summons Number='4006265037', Plate ID='FZX9232', Registration State='NY', Issue Date='2016-08-23', Violation Code='5', Vehicle Body Type='SUBN', Vehicle Make='FORD', Violation Precinct='0', Issuer Precinct='0', Violation Time='0233P')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datafram can be created by by calling read method on spark object\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Date loaded seems to have tickets issued in the year 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: string (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Code: string (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: string (nullable = true)\n",
      " |-- Issuer Precinct: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFame will have columns, and we use a schema to define them.\n",
    "df.printSchema()\n",
    "\n",
    "# printSchema returns schema in tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Summons Number,StringType,true),StructField(Plate ID,StringType,true),StructField(Registration State,StringType,true),StructField(Issue Date,StringType,true),StructField(Violation Code,StringType,true),StructField(Vehicle Body Type,StringType,true),StructField(Vehicle Make,StringType,true),StructField(Violation Precinct,StringType,true),StructField(Issuer Precinct,StringType,true),StructField(Violation Time,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema\n",
    "#Returns the schema of this DataFrame as a pyspark.sql.types.StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mindate='1972-03-30', maxdate='2069-11-19')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register initial DataFrame df as temp table dfTable\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql('SELECT MIN(`Issue Date`) AS mindate, MAX(`Issue Date`) AS maxdate FROM dfTable').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Data loaded has tickets issued across many years. 1972 is the oldest and 2069 is latest. 2069 year does not make any sense. This is bad data.\n",
    "\n",
    "Our focus is to analyze the tickets issued in the year of 2017. Time to clean up the data to ensure that we only analyze tickets issued in the year 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mindate='2017-01-01', maxdate='2017-12-31')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering data for year 2017\n",
    "df_2017_Issued = spark.sql('SELECT * FROM dfTable where year(`Issue Date`) = 2017')\n",
    "df_2017_Issued.createOrReplaceTempView(\"df_2017_Issued_Table\")\n",
    "\n",
    "spark.sql('SELECT MIN(`Issue Date`) AS mindate, MAX(`Issue Date`) AS maxdate FROM df_2017_Issued_Table').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Now the data seems to be limited to only year 2017.\n",
    "\n",
    "We will continue with this data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(summons_count=5431918)]\n",
      "[Row(distinct_summons_count=5431918)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('SELECT count(`Summons Number`) AS summons_count FROM df_2017_Issued_Table').take(1))\n",
    "print(spark.sql('SELECT count(distinct `Summons Number`) AS distinct_summons_count FROM df_2017_Issued_Table').take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: We do not seem to have any duplicate Summons Number. We are good to continue with further analysis.\n",
    "### Examine the data 1. The total number of tickets for the year 2017 is 5431918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tickets issued per state vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of states that have cars from with issued tickets is 65\n",
      "+------------------+---------------+\n",
      "|Registration State|Violation Count|\n",
      "+------------------+---------------+\n",
      "|                NY|        4273951|\n",
      "|                NJ|         475825|\n",
      "|                PA|         140286|\n",
      "|                CT|          70403|\n",
      "|                FL|          69468|\n",
      "|                IN|          45525|\n",
      "|                MA|          38941|\n",
      "|                VA|          34367|\n",
      "|                MD|          30213|\n",
      "|                NC|          27152|\n",
      "|                TX|          18827|\n",
      "|                IL|          18666|\n",
      "|                GA|          17537|\n",
      "|                99|          16055|\n",
      "|                AZ|          12379|\n",
      "+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total number of states that have cars from with issued tickets is ' + str(spark.sql('SELECT count (distinct `Registration State`) no_of_states  FROM df_2017_Issued_Table').first()[0]))\n",
    "spark.sql('SELECT `Registration State`, count(`Registration State`) `Violation Count` FROM df_2017_Issued_Table group by 1 order by 2 desc limit 15').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Total number of states that have issued tickets is 65. Invalid state 99 found issuing 16055 tickets.\n",
    "NY cars having maximum number of tickets issued, tickets with 99 state will be updated to NY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_2017_Issued = df_2017_Issued.withColumn('Registration State', regexp_replace('Registration State', '99', 'NY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|Registration State|Violation Count|\n",
      "+------------------+---------------+\n",
      "|                NY|        4290006|\n",
      "|                NJ|         475825|\n",
      "|                PA|         140286|\n",
      "|                CT|          70403|\n",
      "|                FL|          69468|\n",
      "|                IN|          45525|\n",
      "|                MA|          38941|\n",
      "|                VA|          34367|\n",
      "|                MD|          30213|\n",
      "|                NC|          27152|\n",
      "|                TX|          18827|\n",
      "|                IL|          18666|\n",
      "|                GA|          17537|\n",
      "|                AZ|          12379|\n",
      "|                OH|          12281|\n",
      "+------------------+---------------+\n",
      "\n",
      "Total number of states that have cars from with issued tickets is 64\n"
     ]
    }
   ],
   "source": [
    "df_2017_Issued.createOrReplaceTempView(\"df_2017_Issued_Table\")\n",
    "spark.sql('SELECT `Registration State`, count(`Registration State`) `Violation Count` FROM df_2017_Issued_Table group by 1 order by 2 desc limit 15').show()\n",
    "print('Total number of states that have cars from with issued tickets is ' + str(spark.sql('SELECT count (distinct `Registration State`) no_of_states FROM df_2017_Issued_Table').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: 16055 tickets are now updated to NY state now having 4290006 tickets issued.\n",
    "### Examine the data 2. Total number of states that have cars from with issued tickets is 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 1. Frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|Violation Code|Violation Count|\n",
      "+--------------+---------------+\n",
      "|            21|         768087|\n",
      "|            36|         662765|\n",
      "|            38|         542079|\n",
      "|            14|         476664|\n",
      "|            20|         319646|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select `Violation Code`, count(`Violation Code`) `Violation Count` from df_2017_Issued_Table group by 1 order by 2 desc limit 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 2. How often does each 'vehicle body type' and 'vehicle make' get a parking ticket? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|Vehicle Body Type|Tickets count|\n",
      "+-----------------+-------------+\n",
      "|             SUBN|      1883954|\n",
      "|             4DSD|      1547312|\n",
      "|              VAN|       724029|\n",
      "|             DELV|       358984|\n",
      "|              SDN|       194197|\n",
      "+-----------------+-------------+\n",
      "\n",
      "+------------+-------------+\n",
      "|Vehicle Make|Tickets count|\n",
      "+------------+-------------+\n",
      "|        FORD|       636844|\n",
      "|       TOYOT|       605291|\n",
      "|       HONDA|       538884|\n",
      "|       NISSA|       462017|\n",
      "|       CHEVR|       356032|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select `Vehicle Body Type`, count(`Vehicle Body Type`) `Tickets count` from df_2017_Issued_Table group by 1 order by 2 desc limit 5').show()\n",
    "spark.sql('select `Vehicle Make`, count(`Vehicle Make`) `Tickets count` from df_2017_Issued_Table group by 1 order by 2 desc limit 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|Violation Precinct|Violation Count|\n",
      "+------------------+---------------+\n",
      "|                 0|         925596|\n",
      "|                19|         274445|\n",
      "|                14|         203553|\n",
      "|                 1|         174702|\n",
      "|                18|         169131|\n",
      "+------------------+---------------+\n",
      "\n",
      "+---------------+---------------+\n",
      "|Issuer Precinct|Violation Count|\n",
      "+---------------+---------------+\n",
      "|              0|        1078406|\n",
      "|             19|         266961|\n",
      "|             14|         200495|\n",
      "|              1|         168740|\n",
      "|             18|         162994|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select `Violation Precinct`, count(`Violation Precinct`) `Violation Count` from df_2017_Issued_Table group by 1 order by 2 desc limit 5').show()\n",
    "spark.sql('select `Issuer Precinct`, count(`Issuer Precinct`) `Violation Count` from df_2017_Issued_Table group by 1 order by 2 desc limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|Violation Precinct|Violation Count|\n",
      "+------------------+---------------+\n",
      "|                 0|         925596|\n",
      "|                19|         274445|\n",
      "|                14|         203553|\n",
      "|                 1|         174702|\n",
      "|                18|         169131|\n",
      "|               114|         147444|\n",
      "+------------------+---------------+\n",
      "\n",
      "+---------------+---------------+\n",
      "|Issuer Precinct|Violation Count|\n",
      "+---------------+---------------+\n",
      "|              0|        1078406|\n",
      "|             19|         266961|\n",
      "|             14|         200495|\n",
      "|              1|         168740|\n",
      "|             18|         162994|\n",
      "|            114|         144054|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select `Violation Precinct`, count(`Violation Precinct`) `Violation Count` from df_2017_Issued_Table group by 1 order by 2 desc limit 6').show()\n",
    "spark.sql('select `Issuer Precinct`, count(`Issuer Precinct`) `Violation Count` from df_2017_Issued_Table group by 1 order by 2 desc limit 6').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Precinct 0 assumed as invalid/erraneous, Precinct 19, 14, 1, 18, 114 in the descending order have maximum violations recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 4. Violation code frequencies for three precincts that have issued the most number of tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+\n",
      "|Issuer Precinct|Violation code|Violation count|\n",
      "+---------------+--------------+---------------+\n",
      "|             19|            46|          48445|\n",
      "|             19|            38|          36386|\n",
      "|             19|            37|          36056|\n",
      "|             19|            14|          29797|\n",
      "|             19|            21|          28415|\n",
      "|             19|            20|          14629|\n",
      "|             19|            40|          11416|\n",
      "|             19|            16|           9926|\n",
      "|             19|            71|           7493|\n",
      "|             19|            19|           6856|\n",
      "+---------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------+--------------+---------------+\n",
      "|Issuer Precinct|Violation code|Violation count|\n",
      "+---------------+--------------+---------------+\n",
      "|             14|            14|          45036|\n",
      "|             14|            69|          30464|\n",
      "|             14|            31|          22555|\n",
      "|             14|            47|          18364|\n",
      "|             14|            42|          10027|\n",
      "|             14|            46|           7679|\n",
      "|             14|            19|           7031|\n",
      "|             14|            84|           6743|\n",
      "|             14|            82|           5052|\n",
      "|             14|            40|           3582|\n",
      "+---------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------+--------------+---------------+\n",
      "|Issuer Precinct|Violation code|Violation count|\n",
      "+---------------+--------------+---------------+\n",
      "|              1|            14|          38354|\n",
      "|              1|            16|          19081|\n",
      "|              1|            20|          15408|\n",
      "|              1|            46|          12745|\n",
      "|              1|            38|           8535|\n",
      "|              1|            17|           7526|\n",
      "|              1|            37|           6470|\n",
      "|              1|            31|           5853|\n",
      "|              1|            69|           5672|\n",
      "|              1|            19|           5375|\n",
      "+---------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select `Issuer Precinct`, `Violation code`, count(`Violation code`) `Violation count` from df_2017_Issued_Table group by 1, 2 order by 3 desc').where(\"`Issuer Precinct` in ('19')\").show(10)\n",
    "spark.sql('select `Issuer Precinct`, `Violation code`, count(`Violation code`) `Violation count` from df_2017_Issued_Table group by 1, 2 order by 3 desc').where(\"`Issuer Precinct` in ('14')\").show(10)\n",
    "spark.sql('select `Issuer Precinct`, `Violation code`, count(`Violation code`) `Violation count` from df_2017_Issued_Table group by 1, 2 order by 3 desc').where(\"`Issuer Precinct` in ('1')\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    " - Violation Code **14** seems to be the most common across all the top 3 Precincts.\n",
    " - Precinct 19 has 46, 38, 37, **14**, 21 Violation codes in the descending order of most common.\n",
    " - Precinct 14 has **14**, 69, 31, 47, 42 Violation codes in the descending order of most common.\n",
    " - Precinct 1 has **14**, 16, 20, 46, 38 Violation codes in the descending order of most common.\n",
    " - Violation code **19** does not seem to be occuring too often when compared to the most common violation across all the top 3 precincts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 5. Properties of parking violations across different times of the day.\n",
    "Taking out the hour from the violation time and picking only rows which have valid data i.e. violation time < 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|vtime|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|   11|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|         0852P|   20|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "timedf = spark.sql(\"select *, case when substr(`Violation Time`,5,1) = 'P' and substr(`Violation Time`,0,2) <> '12' then int(substr(`Violation Time`,0,2) + 12) else int(substr(`Violation Time`,0,2)+0) end as vtime from df_2017_Issued_Table\").where(\"substr(`Violation Time`,1,2) < 13\")\n",
    "timedf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|vtime|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timedf.filter(\"`Violation Time` is null\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Null values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|vtime|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|   11|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13|             7|             SUBN|       ME/BE|                 0|              0|         0852P|   20|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11|            78|             DELV|       FRUEH|               106|            106|         0015A|    0|\n",
      "|    1413656420|T672371C|                NY|2017-02-04|            40|             TAXI|       TOYOT|                73|             73|         0525A|    5|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26|            64|              VAN|       INTER|                17|             17|         0256P|   14|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timedf.createOrReplaceTempView(\"df_2017_timedf_Table\");\n",
    "spark.sql('select * from df_2017_timedf_Table').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking which code falls under what time of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_timeOfDay   =   spark.sql(\"\"\"select  *, \n",
    "                                case \n",
    "                                    when vtime between 0 and 3 \n",
    "                                    then 'Mid Night' \n",
    "                                    when vtime between 4 and 7 \n",
    "                                    then 'Early Morning' \n",
    "                                    when vtime between 8 and 11\n",
    "                                    then 'Late morning' \n",
    "                                    when vtime between 12 and 15\n",
    "                                    then 'Afternoon'\n",
    "                                    when vtime between 16 and 19\n",
    "                                    then 'Evening'\n",
    "                                    when vtime between 20 and 23\n",
    "                                    then 'Night'\n",
    "                                    else 'Unknown'\n",
    "                                end as TimeOFDay\n",
    "                                from df_2017_timedf_Table\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_timeOfDay.createOrReplaceTempView(\"df_2017_timeOfDay_Table\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|    TimeOFDay|Violation Count|\n",
      "+-------------+---------------+\n",
      "| Late morning|        2163568|\n",
      "|    Afternoon|        1857194|\n",
      "|      Evening|         637540|\n",
      "|Early Morning|         449885|\n",
      "|        Night|         176360|\n",
      "|    Mid Night|         147300|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select  TimeOFDay, count(1) `Violation Count` from df_2017_timeOfDay_Table group by 1 order by 2 desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------------------+\n",
      "|   TimeOFDay|Violation Code|count(Violation Code)|\n",
      "+------------+--------------+---------------------+\n",
      "|Late morning|            21|               598070|\n",
      "+------------+--------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+--------------+---------------------+\n",
      "|   TimeOFDay|Violation Code|count(Violation Code)|\n",
      "+------------+--------------+---------------------+\n",
      "|Late morning|            36|               348165|\n",
      "+------------+--------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+--------------+---------------------+\n",
      "|TimeOFDay|Violation Code|count(Violation Code)|\n",
      "+---------+--------------+---------------------+\n",
      "|Afternoon|            38|               240795|\n",
      "+---------+--------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select TimeOFDay,`Violation Code`, count(`Violation Code`) from df_2017_timeOfDay_Table group by 1,2 order by 3 desc').where(\"`Violation Code` in ('21')\").show(1)\n",
    "spark.sql('select TimeOFDay,`Violation Code`, count(`Violation Code`) from df_2017_timeOfDay_Table group by 1,2 order by 3 desc').where(\"`Violation Code` in ('36')\").show(1)\n",
    "spark.sql('select TimeOFDay,`Violation Code`, count(`Violation Code`) from df_2017_timeOfDay_Table group by 1,2 order by 3 desc').where(\"`Violation Code` in ('38')\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Across the time of the day, Mostly violation occurs in the late morning, that is from 8 AM to 12 PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 6. Seasonality in the data.\n",
    "Dividing the data into real seasons to see if that will identify any pattern. \n",
    "\n",
    " - Winter Season - December, January, February\n",
    " - Spring Season - March, April, May\n",
    " - Summer Season - June, July, August\n",
    " - Fall Season - September, October, November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|Season|Violation Count|\n",
      "+------+---------------+\n",
      "|Spring|        2873340|\n",
      "|Winter|        1704673|\n",
      "|Summer|         852855|\n",
      "|  Fall|            979|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2017_seasonal = spark.sql(\"\"\"select  *, \n",
    "                                case \n",
    "                                    when month(`Issue Date`) = 12 or\n",
    "                                         month(`Issue Date`) = 1 or \n",
    "                                         month(`Issue Date`) = 2 \n",
    "                                    then 'Winter' \n",
    "                                    when month(`Issue Date`) = 3 or \n",
    "                                         month(`Issue Date`) = 4 or \n",
    "                                         month(`Issue Date`) = 5 \n",
    "                                    then 'Spring' \n",
    "                                    when month(`Issue Date`) = 6 or \n",
    "                                         month(`Issue Date`) = 7 or \n",
    "                                         month(`Issue Date`) = 8 \n",
    "                                    then 'Summer' \n",
    "                                    when month(`Issue Date`) = 9 or\n",
    "                                         month(`Issue Date`) = 10 or\n",
    "                                         month(`Issue Date`) = 11\n",
    "                                    then 'Fall'\n",
    "                                    else 'Unknown'\n",
    "                                end as Season\n",
    "                                from df_2017_timeOfDay_Table\"\"\");\n",
    "df_2017_seasonal.createOrReplaceTempView(\"df_2017_Seasonal_Table\");\n",
    "\n",
    "spark.sql('select  Season, count(1) `Violation Count` from df_2017_Seasonal_Table group by 1 order by 2 desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    " - No data found for 'Unknown'. Hence no bad dates.\n",
    " - **Spring** season followed by **Winter** season seem to have maximum number of violations. These seasons are known for cold climate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------+\n",
      "|Season|Violation Code|Violation Count|\n",
      "+------+--------------+---------------+\n",
      "|Spring|            21|         402408|\n",
      "|Spring|            36|         344834|\n",
      "|Spring|            38|         271167|\n",
      "|Spring|            14|         256396|\n",
      "|Spring|            46|         173437|\n",
      "+------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------------+---------------+\n",
      "|Season|Violation Code|Violation Count|\n",
      "+------+--------------+---------------+\n",
      "|Winter|            21|         238180|\n",
      "|Winter|            36|         221268|\n",
      "|Winter|            38|         187385|\n",
      "|Winter|            14|         142262|\n",
      "|Winter|            20|          97996|\n",
      "+------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------------+---------------+\n",
      "|Season|Violation Code|Violation Count|\n",
      "+------+--------------+---------------+\n",
      "|Summer|            21|         127347|\n",
      "|Summer|            36|          96663|\n",
      "|Summer|            38|          83518|\n",
      "|Summer|            14|          77911|\n",
      "|Summer|            20|          64473|\n",
      "+------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+--------------+---------------+\n",
      "|Season|Violation Code|Violation Count|\n",
      "+------+--------------+---------------+\n",
      "|  Fall|            46|            231|\n",
      "|  Fall|            21|            128|\n",
      "|  Fall|            40|            116|\n",
      "|  Fall|            14|             94|\n",
      "|  Fall|            19|             57|\n",
      "+------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select  Season, `Violation Code`, count(1) `Violation Count` from df_2017_Seasonal_Table group by 1, 2 order by 3 desc').where(\"Season = 'Spring'\").show(5)\n",
    "spark.sql('select  Season, `Violation Code`, count(1) `Violation Count` from df_2017_Seasonal_Table group by 1, 2 order by 3 desc').where(\"Season = 'Winter'\").show(5)\n",
    "spark.sql('select  Season, `Violation Code`, count(1) `Violation Count` from df_2017_Seasonal_Table group by 1, 2 order by 3 desc').where(\"Season = 'Summer'\").show(5)\n",
    "spark.sql('select  Season, `Violation Code`, count(1) `Violation Count` from df_2017_Seasonal_Table group by 1, 2 order by 3 desc').where(\"Season = 'Fall'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "- Across the **Spring/Winter/Summer** seasons, Violation codes 21, 36, 38  in descending order seem to the most common.\n",
    "- Violation codes 21, 14 also are among the top few in Fall season.\n",
    "- In **Fall** season, Violation Codes 46, 21, 40 in descending order seem to be the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation 7. \n",
    "### Finding the total occurrences of the three most common violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|Violation code|count(Violation code)|\n",
      "+--------------+---------------------+\n",
      "|            21|               768063|\n",
      "|            36|               662765|\n",
      "|            38|               542078|\n",
      "+--------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select  `Violation code`, count(`Violation code`) from df_2017_Seasonal_Table group by 1 order by 2 desc').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing fines associated with different violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fine for violation code 21 : $55.0\n",
      "Toatl  count for violation code 21 : 768063\n",
      "Total fine for  violation code 21 : $42243465.0\n"
     ]
    }
   ],
   "source": [
    "# Compute for total fine amount collected for vioaltaion code 21 \n",
    "\n",
    "high_dens_fine_viol_code_21 = 65\n",
    "others_fine_viol_code_21 = 45\n",
    "\n",
    "avg_fine_viol_code_21 = (high_dens_fine_viol_code_21 + others_fine_viol_code_21)/2\n",
    "print ('Average fine for violation code 21 : $' + str(avg_fine_viol_code_21))\n",
    "\n",
    "Total_count_viol_code_21  = spark.sql('select  `Violation code`, count(`Violation code`)  from df_2017_Seasonal_Table group by 1 order by 2 desc').where(\"`Violation code` in ('21')\").first()[1]\n",
    "print ('Toatl  count for violation code 21 : ' + str(Total_count_viol_code_21))\n",
    "\n",
    "Total_fine_viol_code_21 = Total_count_viol_code_21*avg_fine_viol_code_21\n",
    "print ('Total fine for  violation code 21 : $' + str(Total_fine_viol_code_21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fine for violation code 36 : $50.0\n",
      "Toatl  count for violation code 36 : 662765\n",
      "Total fine for  violation code 36 : $33138250.0\n"
     ]
    }
   ],
   "source": [
    "# Compute for total fine amount collected for vioaltaion code 36 \n",
    "\n",
    "high_dens_fine_viol_code_36 = 50\n",
    "others_fine_viol_code_36 = 50\n",
    "\n",
    "avg_fine_viol_code_36 = (high_dens_fine_viol_code_36 + others_fine_viol_code_36)/2\n",
    "print ('Average fine for violation code 36 : $' + str(avg_fine_viol_code_36))\n",
    "\n",
    "Total_count_viol_code_36  = spark.sql('select  `Violation code`, count(`Violation code`)  from df_2017_Seasonal_Table group by 1 order by 2 desc').where(\"`Violation code` in ('36')\").first()[1]\n",
    "print ('Toatl  count for violation code 36 : ' + str(Total_count_viol_code_36))\n",
    "\n",
    "Total_fine_viol_code_36 = Total_count_viol_code_36*avg_fine_viol_code_36\n",
    "print ('Total fine for  violation code 36 : $' + str(Total_fine_viol_code_36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fine for violation code 38 : $50.0\n",
      "Toatl  count for violation code 38 : 542078\n",
      "Total fine for  violation code 38 : $27103900.0\n"
     ]
    }
   ],
   "source": [
    "# Compute for total fine amount collected for vioaltaion code 38 \n",
    "\n",
    "high_dens_fine_viol_code_38 = 65\n",
    "others_fine_viol_code_38 = 35\n",
    "\n",
    "avg_fine_viol_code_38 = (high_dens_fine_viol_code_38 + others_fine_viol_code_38)/2\n",
    "print ('Average fine for violation code 38 : $' + str(avg_fine_viol_code_38))\n",
    "\n",
    "Total_count_viol_code_38  = spark.sql('select  `Violation code`, count(`Violation code`)  from df_2017_Seasonal_Table group by 1 order by 2 desc').where(\"`Violation code` in ('38')\").first()[1]\n",
    "print ('Toatl  count for violation code 38 : ' + str(Total_count_viol_code_38))\n",
    "\n",
    "Total_fine_viol_code_38 = Total_count_viol_code_38*avg_fine_viol_code_38\n",
    "print ('Total fine for  violation code 38 : $' + str(Total_fine_viol_code_38))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Observation : The code that has the highest total collection is 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### We can Infer from the above  findings :\n",
    " #### The violation code 21 results highest average penalty/fine  among top three viaolation code \n",
    " #### also the highest total  collection genetares  from violation code 21\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
